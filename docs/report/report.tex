%% vi: set tabstop=2, set textwidth=80
\documentclass[a4paper,11pt]{article}

\usepackage{homework}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[english]{babel}

\title{Locate My Plate \\ A License Plate Localisation System}

\date{June 24, 2009}

\begin{document}

\maketitle
\section*{Introduction}
This report describes the implementation of a robust, real-time License Plate
Recognition System (LPR). Using supervised learning, the system generates a
cascading classifier which consists of multiple layers that each holds one
strong classifier. These strong classifiers are a linear function of several
weak classifiers aka features which each describe characteristics of a license
plate. The first section describes the implementation of the algorithms
involved, next we show our results using an american dataset and finally we
conclude.


\subsection{Features generation}
	\subsection{Introduction}
	A feature can be defined as a scanning window contaning a set of ascending
	segments. The segments which form a group are called the feature blocks. Each
	feature block takes the sum of all pixels in that block. This is done very
	efficient using integral images \cite{viola}. The feature blocks ar summed up
	or substracted depended on their sign.\\

	Because the segments and feature blocks are vertically separated, the
	feature is called a vertical feature.  In addition horizontal features are
	generated using the transpose of the vertical features.\\

	A feature is represented as a binary code. Each element in the binary
	represents the position and the sign of a feature segment.  This
	representation makes it easy to generate all possible features permutations
	using the powerset.  Because half of the features are simply the inverse of
	another feature, the set is pruned.

	\subsection{Optimization}
	Lots of features share the same feature block dimensions. Even a feature
	itself could have more feature blocks of the same dimensions. Because the
	feature is used as a scanning window, lots of redundant calculations are done.

	To optimize this, the feature blocks are individually calculated for every
	position (i,j) in the image and stored on its dimensions (h,w) in a hash
	table R.\\
	The matrices are alligned depending on the position of the block in the
	feature and added up. This gives the total value of a feature (again for
	every position, stored in a matrix).

	\section{Image types}
	The features where applied to the following image types.
	\begin{itemize}
		\item{1th order derivative in both x and y directions.}
		\item{2nd order derivative in both x and y directions.}
		\item{variance in both x and y directions.}
	\end{itemize}
	Before applying the feature, the above image types are passed through an
	absolute filter.


\section{Training}
	\subsection*{Training strong classifiers}
	TODO adaboost comes here

	\subsection{Cascading classifier}


\section*{Results}
\subsection*{Dataset}
The dataset is with permission obtained from \cite{dlagnekov_dataset}. It
contains 293 car images annotated on location and size of the license plate.
The dataset is divided in a train-, test- and evaluation set with respectively
199,47 and 47 images.


\subsection*{
confusion matrix
info about datasets, nr examples, train test etc.
performance cascader displayed in graph (on every layer display the exclusions)
best features on every layer
(optional endresult, car with rectangle at license plate)
(optionoal prob image of a good performing feature)


\section*{Conclusions}
TODO ispell

\renewcommand\bibname{References}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{document}
