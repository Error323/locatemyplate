%% vi: set tabstop=2, set textwidth=80
\documentclass[a4paper,11pt]{article}

\usepackage{homework}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[english]{babel}

\title{Locate My Plate \\ A License Plate Localisation System}

\date{June 24, 2009}

\begin{document}

\maketitle
\section*{Introduction}
This report describes the implementation of a robust, real-time License Plate
Localisation System (LPR). Using supervised learning, the system generates a
cascading classifier which consists of multiple layers that each holds one
strong classifier. These strong classifiers are a linear function of several
weak classifiers aka features which each describe characteristics of a license
plate. The first section describes the implementation of the algorithms
involved, next the results using an american dataset are shown and finally the
conclusion. \cite{viola}


\subsection{Features generation}
\section*{integral images}
how is a integral image computed, TODO copy from other paper, do not invent the
wheel :P

	\subsection{introduction}
	A feature can be defined as a scanning window contaning a set of ascending
	segments. The segments which form a group are called the feature blocks. Each
	feature block takes the sum of all pixels the block. This is done very
	efficient using integral images.  TODO (reference integral images). Depended
	of the sign of the individual featureblocks he is summed up or substracted.\\ 

	Because the segments and featureblocks are vertically seperated, the
	feature is called a vertical feature.  The horizontal features are simply
	generated using the transpose of this vertical feature.\\

	A feature can be represented as a binary code. Each element in the binary
	represents the position and the sign of a segment of a feature.
	example TODO picture\\

	This representation makes it easy to generate all possible features
	permutations using the powerset.  Because half of the features are simply
	the inverse of another feature the set is pruned.

	\subsection{optimalisation}
	Lots of features share the same featureblock dimensions. Even a feature
	itself could have more featureblocks of the same dimensions.  Because the
	feature is used as a scanning window lots of unnecessary redundant
	calculations are done.

	To optimize this, the featureblocks are individually calculated for every
	position (i,j) in the image and stored on its dimensions (h,w) in a hash
	table R.  For example R(3,4) contains the matrix for featureblock with
	height=3 and width=4.  R(3,4)(20,4) contains the value of position (20,4) in
	the image for this featureblock.

	The matrices are alligned depending on the position of the block in the
	feature and added up. This gives the total value of a feature (again for
	every position, stored in a matrix).

	\section{image types}
	The features where applied to the following integral image types.
	\begin{itemize}
		\item{1th order derivative in both x and y directions.}
		\item{2nd order derivative in both x and y directions.}
		\item{variance in both x and y directions.}
	\end{itemize}
	Before applying the feature the above integral image types are passed through an absolute filter.


\section*{Training}
The overall method consists of three types of training. The first type is the
training of the weak classifiers or features. The second type is a linear
combination of one or more weak classifiers into a strong classifier using a
boosting algorithm. Finally the type stage is a cascading classifier with a
strong classifier on each layer.
\subsection*{Weak classifier}
A weak classifier or feature is trained by determining a threshold $t$ which
seperates positive and negative samples as good as possible. See Algorithm
\ref{alg:weak}.
\begin{algorithm}
	\caption{trainWeakClassifier($C$, $P$, $N$): Determines a threshold $t$ which
	separets positive and negative examples as good as possible.}
	\begin{algorithmic}[1]
	\REQUIRE $C$ the weak classifier, $P$ positive samples, $N$ negative samples
	\medskip
	\STATE Let $V$ be a table $V = x|x_v|x_s$, where $x \in P \cup N$, $x_v = C(x)$ and $x_s \in {0,1}$.
	\STATE Sort $V$ on $x_v$
	\FOR {$i = 2 to |V|-1$}
		\STATE 
	\ENDFOR
	
	\end{algorithmic}
\label{alg:weak}
\end{algorithm}
\subsection*{Strong classifier}
\subsection*{Cascading classifier}


\section*{Dataset}
\section*{Results}
confusion matrix
info about datasets, nr examples, train test etc.
performance cascader displayed in graph (on every layer display the exclusions)
best features on every layer
(optional endresult, car with rectangle at license plate)
(optionoal prob image of a good performing feature)


\section*{Conclusions}
TODO ispell

\renewcommand\bibname{References}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{document}
