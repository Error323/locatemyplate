%% vi: set tabstop=2, set textwidth=80
\documentclass[a4paper,11pt]{article}

\usepackage{homework}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[english]{babel}

\title{Locate My Plate \\ A License Plate Localisation System}

\date{June 24, 2009}

\begin{document}

\maketitle
\section*{Introduction}
This report describes the implementation of a robust, real-time License Plate
Localisation System (LPR). Using supervised learning, the system generates a
cascading classifier which consists of multiple layers that each holds one
strong classifier. These strong classifiers are a linear function of several
weak classifiers aka features which each describe characteristics of a license
plate. The first section describes the implementation of the algorithms
involved, next the results using an american dataset are shown and finally the
conclusion.


\subsection*{Features generation}
	\subsection*{Introduction}
	A feature can be defined as a scanning window contaning a set of ascending
	segments. The segments which form a group are called the feature blocks. Each
	feature block takes the sum of all pixels in that block. This is done very
	efficient using integral images \cite{viola}. The feature blocks ar summed up
	or substracted depended on their sign.\\

	Because the segments and feature blocks are vertically separated, the
	feature is called a vertical feature.  In addition horizontal features are
	generated using the transpose of the vertical features.\\

	A feature is represented as a binary code. Each element in the binary
	represents the position and the sign of a feature segment.  This
	representation makes it easy to generate all possible features permutations
	using the powerset.  Because half of the features are simply the inverse of
	another feature, the set is pruned.

	\subsection*{Optimization}
	Lots of features share the same feature block dimensions. Even a feature
	itself could have more feature blocks of the same dimensions. Because the
	feature is used as a scanning window, lots of redundant calculations are done.

	To optimize this, the feature blocks are individually calculated for every
	position (i,j) in the image and stored on its dimensions (h,w) in a hash
	table R.\\
	The matrices are alligned depending on the position of the block in the
	feature and added up. This gives the total value of a feature (again for
	every position, stored in a matrix).

	\section*{Image types}
	The features where applied to the following image types.
	\begin{itemize}
		\item{1th order derivative in both x and y directions.}
		\item{2nd order derivative in both x and y directions.}
		\item{variance in both x and y directions.}
	\end{itemize}
	Before applying the feature, the above image types are passed through an
	absolute filter.


\section*{Training}
The overall method consists of three types of training. The first type is the
training of the weak classifiers or features. The second type is a linear
combination of one or more weak classifiers into a strong classifier using a
boosting algorithm. Finally the type stage is a cascading classifier with a
strong classifier on each layer.

\subsection*{Weak classifier}
A weak classifier is a feature with a threshold $t \in \mathbb{R}$ and an
operator $\circ \in \{<, >\}$ which separates positive and negative samples as
good as possible according to the trainings set. After training, the weak
classifier $C$ constructs a binary image $B = t \circ F(x)$, where $x$ is the
image and $F$ the function of the feature. This binary image $B$ contains
possible license plates according to the feature.

\subsection*{Strong classifier}
A strong classifier is constructed according to the boosting algorithm
described by Viola and Jones \cite{viola}.  By re-weighting the positive and
negative samples after selecting a weakClassifier, the algorithm selects the
"best" features with their respective alpha. Classification is performed as
follows:
\begin{displaymath}
C(x) = 
	\left\{ \begin{array}{ll}
		1 & \sum^T_{t=1} \alpha_t W_t(x) \ge \frac{1}{2} \sum^T_{t=1}\alpha_t \\
		0 & \textrm{otherwise}
	\end{array} \right.
\end{displaymath}
where $T$ is the number of weak classifiers and $W_t$ a function which returns
the binary image as described in the Weak classifier section.

\subsection*{Cascading classifier}
The cascading classifier is the final classifier. It consists of multiple
layers where each layer represents a strong classifier. This classifier is
trained as described by Viola and Jones \cite{viola}. 
\begin{algorithm}
	\caption{classify($C$, $x$): Returns the binary image $B$ of $x$}
	\begin{algorithmic}[1]
	\REQUIRE $C$ the cascading classifier, $x$ the image
	\medskip
	\STATE Let $B$ be an image with the same dimensions as $x$ of ones.
	\FOR {$c_s \in C$}
		\STATE $B' \leftarrow c_s(x)$
		\STATE $B \leftarrow B \land B'$
	\ENDFOR
	\RETURN $B$
	\end{algorithmic}
\label{alg:casc}
\end{algorithm}
Algorithm \ref{alg:casc} shows the classification of an image using a trained
cascading classifier.

\section*{Dataset}
The dataset is with permission obtained from \cite{dlagnekov_dataset}. It
contains 293 car images annotated on location and size of the license plate.
The dataset is divided in a train-, test- and validation set with respectively
199,47 and 47 images.

\section*{Results}
confusion matrix
info about datasets, nr examples, train test etc.
performance cascader displayed in graph (on every layer display the exclusions)
best features on every layer
(optional endresult, car with rectangle at license plate)
(optionoal prob image of a good performing feature)


\section*{Conclusions}
TODO ispell

\renewcommand\bibname{References}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{document}

